version: "3.9"
volumes:
  postgres_data:
    driver: local
  web:
    driver: local
secrets:
  db_login:
    file: ./db_login.txt
  db_password:
    file: ./db_password.txt
services:
  db:
    image: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    secrets:
      - db_login
      - db_password
    environment:
      - POSTGRES_DB
      - POSTGRES_USER_FILE=/run/secrets/db_login
      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password
    ports:
      - "5432:5432"  # need to open DB outside container to let DBeaver access it
  web:
    build: .  # act on Dockerfile in current dir
    ports:
      - "8000"
    environment:  &app_envvars
      - ALLOWED_HOSTS  # values will come from .env
      - CELERY_BROKER_URL
      - DJANGO_SUPERUSER_EMAIL
      - SECRET_KEY
      - EMAIL_HOST
      - EMAIL_PORT
      - EMAIL_HOST_USER
      - EMAIL_HOST_PASSWORD
      - EMAIL_USE_TLS
      - POSTGRES_DB
      - POSTGRES_HOST
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - PYTHONDEBUG
      - PYTHONDONTWRITEBYTECODE=1  # no .pyc
      - PYTHONUNBUFFERED=1 # no buffering stdout/stderr so Django logs are real time into container console
    depends_on:
      - db  # cannot start if db service is not up and running
      - broker
  mail:
    image: "mailhog/mailhog:v1.0.1"
    ports:
      - "1025:1025"
      - "8025:8025"
  broker:
    image: "rabbitmq:3.9.13-management"
    environment:
      - RABBITMQ_DEFAULT_USER
      - RABBITMQ_DEFAULT_PASS
    ports:
      - "5672:5672"    # queue
      - "15672:15672"  # Web UI: http://container-ip:15672
  worker:
    build:
      context: .
    command: celery -A tasks worker --loglevel=info
    environment:  *app_envvars
    depends_on:
      - broker
  nginx:
    build:
      context: .
      dockerfile: ./nginx/Dockerfile
    volumes:
      - web:/www
    ports:
      - "8007:80"
    depends_on:
      - web
